{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- movie_lines.txt\n",
    "\t- contains the actual text of each utterance\n",
    "\t- fields:\n",
    "\t\t- lineID\n",
    "\t\t- characterID (who uttered this phrase)\n",
    "\t\t- movieID\n",
    "\t\t- character name\n",
    "\t\t- text of the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = open(\"cornell movie-dialogs corpus/movie_lines.txt\").read().split('\\n')\n",
    "movie_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- movie_conversations.txt\n",
    "\t- the structure of the conversations\n",
    "\t- fields\n",
    "\t\t- characterID of the first character involved in the conversation\n",
    "\t\t- characterID of the second character involved in the conversation\n",
    "\t\t- movieID of the movie in which the conversation occurred\n",
    "\t\t- list of the utterances that make the conversation, in chronological \n",
    "\t\t\torder: ['lineID1','lineID2',Ã‰,'lineIDN']\n",
    "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conv = open(\"cornell movie-dialogs corpus/movie_conversations.txt\").read().split('\\n')\n",
    "movie_conv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower too\n",
    "indi_line = {}\n",
    "for dia in movie_lines:\n",
    "    indi_line[dia.split(' +++$+++ ')[0]] = dia.split(' +++$+++ ')[-1].lower()  \n",
    "\n",
    "print(\"any 5 individual lines\")\n",
    "dict(islice(indi_line.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = []\n",
    "for conv in movie_conv:\n",
    "    i = []\n",
    "    p = conv.split(' +++$+++ ')[-1]   \n",
    "    for m in re.findall(r\"[\\w']+\", p):\n",
    "        i.append(m.split('\\'')[1])\n",
    "    conversation.append(i)\n",
    "conversation[:5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_char(word):\n",
    "    word = re.sub(r\"i'm\", \"i am\", word)\n",
    "    word = re.sub(r\"you're\", \"you are\", word)\n",
    "    word = re.sub(r\"don't\", \"do not\", word)\n",
    "    word = re.sub(r\"you'd\", \"you would\", word)\n",
    "    word = re.sub(r\"i've\", \"i have\", word)\n",
    "    word = re.sub(r\"can't\", \"can not\", word)\n",
    "    word = re.sub(r\"didn't\", \"did not\", word)\n",
    "    word = re.sub(r\"aren't\", \"are not\", word)\n",
    "    word = re.sub(r\"it's\", \"it is\", word)\n",
    "    word = re.sub(r\"i'd\", \"i would\", word)\n",
    "    word = re.sub(r\"isn't\", \"is not\", word)\n",
    "    word = re.sub(r\"we'd\", \"we had\", word)\n",
    "    word = re.sub(r\"won't\", \"will not\", word)\n",
    "    word = re.sub(r\"you'll\", \"you will\", word)\n",
    "    word = re.sub(r\"couldn't\", \"could not\", word)\n",
    "    word = re.sub(r\"could've\", \"could have\", word)\n",
    "    word = re.sub(r\"hasn't\", \"has not\", word)\n",
    "    word = re.sub(r\"how's\", \"how is\", word)\n",
    "    word = re.sub(r\"goin'\", \"going\", word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dealing with words that contain apostrophe using special_char method\n",
    "   Also deleting any other symbol present'''\n",
    "\n",
    "individual_line1 = {}\n",
    "for key,line in indi_line.items():\n",
    "    diag = \"\"\n",
    "    for p in line.split(' '):\n",
    "        diag+= special_char(p)+\" \"\n",
    "        individual_line1[key]=diag.strip()\n",
    "\n",
    "individual_line = {}\n",
    "for key,line in individual_line1.items():\n",
    "    diag = \"\"\n",
    "    for p in line.split():\n",
    "        diag+= re.sub('[^a-zA-Z]+', '', p)+\" \"\n",
    "        \n",
    "    individual_line[key]=diag.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(islice(individual_line.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_length = []\n",
    "max_len = 0\n",
    "for line in individual_line.values():\n",
    "    count = 0\n",
    "    for word in line.split():\n",
    "        check_length.append(word)\n",
    "        count+=1\n",
    "    if count>max_len:\n",
    "        max_len = count\n",
    "        \n",
    "check_length = set(check_length)\n",
    "print(\"Vocabulary length\",len(check_length))\n",
    "print(\"Maximum movie line length\",max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the length 555 is very large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "min_len = 5\n",
    "final_conv = []\n",
    "for conv in conversation:\n",
    "    c = []\n",
    "    for line in conv:\n",
    "        if len(individual_line[line].split())<max_len and len(individual_line[line].split())>min_len:           \n",
    "            c.append(individual_line[line])\n",
    "    if(len(c)!=0):\n",
    "        final_conv.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_conv[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = []\n",
    "\n",
    "for conv in final_conv:\n",
    "    for line in conv:\n",
    "        for word in line.split():\n",
    "            word2idx.append(word)\n",
    "        \n",
    "word2idx = sorted(set(word2idx))\n",
    "print(\"Vocabulary length\",len(word2idx))\n",
    "word2idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "for index,value in enumerate(word2idx):\n",
    "    idx2word[index] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_conversation = []\n",
    "for conv in tqdm(final_conv):\n",
    "    c = []\n",
    "    for line in conv:\n",
    "        p = []\n",
    "        for word in line.split():\n",
    "            p.append(word2idx.index(word))\n",
    "        c.append(p)\n",
    "    index_conversation.append(c)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx.append('<PAD>')\n",
    "idx2word[word2idx.index('<PAD>')] = '<PAD>'\n",
    "\n",
    "word2idx.append('<EOS>')\n",
    "idx2word[word2idx.index('<EOS>')] = '<EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word2idx.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(word2idx, encoded_pickle)\n",
    "    \n",
    "with open(\"idx2word.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(idx2word, encoded_pickle)\n",
    "    \n",
    "with open(\"index_conversation.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(index_conversation, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = pickle.load(open('word2idx.p', 'rb'))\n",
    "idx2word = pickle.load(open('idx2word.p', 'rb'))\n",
    "index_conversation = pickle.load(open('index_conversation.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_conv[0], end = '\\n\\n')\n",
    "print(index_conversation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_question = []\n",
    "indexed_answer = []\n",
    "\n",
    "for conv in index_conversation:\n",
    "    for i in range(len(conv)-1):\n",
    "        indexed_question.append(conv[i])\n",
    "        indexed_answer.append(conv[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete first Conversation :\", index_conversation[0],end='\\n\\n')\n",
    "print(\"First two questions :\",indexed_question[:2],end='\\n\\n')\n",
    "print(\"First two answers :\",indexed_answer[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple words, in a 'conversation', the first line is the question and the answer is the second line, similarly then the second line becomes the question and the third line becomes the answer and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"indexed_question.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(indexed_question, encoded_pickle)\n",
    "with open(\"indexed_answer.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(indexed_answer, encoded_pickle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_question = pickle.load(open('indexed_question.p', 'rb'))\n",
    "indexed_answer = pickle.load(open('indexed_answer.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_indexed_ques = []\n",
    "final_indexed_target = []\n",
    "pad_id = word2idx.index('<PAD>')\n",
    "eos_id = word2idx.index('<EOS>')\n",
    "max_len = 23   #increasing the size due to the addition of PAD and EOS\n",
    "\n",
    "for i in indexed_question:\n",
    "    i = i + ([pad_id] * (max_len - len(i)))\n",
    "    final_indexed_ques.append(i)\n",
    "    \n",
    "\n",
    "for i in indexed_answer:\n",
    "    i = i + [eos_id] + ([pad_id] * (max_len - len(i) - 1 )) #minus 1 because eos_id is added\n",
    "    final_indexed_target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_indexed_ques.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(final_indexed_ques, encoded_pickle)\n",
    "\n",
    "with open(\"final_indexed_target.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(final_indexed_target, encoded_pickle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_indexed_ques = pickle.load(open('final_indexed_ques.p', 'rb'))\n",
    "final_indexed_target = pickle.load(open('final_indexed_target.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Indexed Question in words\\n')\n",
    "for word_in_index in final_indexed_ques[0]:\n",
    "    print(idx2word[word_in_index], end=' ')\n",
    "\n",
    "print(\"\\nFinal Indexed Target\")\n",
    "for word_in_index in final_indexed_target[0]:\n",
    "    print(idx2word[word_in_index], end=' ')\n",
    "print(\"\\n\",final_indexed_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data_split = [0.7,0.3]\n",
    "total_length = len(final_indexed_ques)\n",
    "\n",
    "split_lengths = [int(s*total_length) for s in data_split]\n",
    "train_en_sequence, train_de_input, train_target = np.array(final_indexed_ques[:split_lengths[0]]), np.array(final_indexed_ans[:split_lengths[0]]), np.array(final_indexed_target[:split_lengths[0]])\n",
    "\n",
    "test_en_sequence, test_de_input, test_target = np.array(final_indexed_ques[-split_lengths[1] : ]) , np.array(final_indexed_ans[-split_lengths[1]:]), np.array(final_indexed_target[-split_lengths[1]:])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch\n",
    "batch_size = 15\n",
    "train_en_sequence_batches = []\n",
    "train_target_batches = []\n",
    "\n",
    "for batch_index in range(len(final_indexed_ques)//batch_size):\n",
    "    train_en_sequence_batches.append(final_indexed_ques[(batch_index*batch_size):(batch_index*batch_size+batch_size)])\n",
    "    train_target_batches.append(final_indexed_target[(batch_index*batch_size):(batch_index*batch_size+batch_size)])\n",
    "    \n",
    "train_en_sequence_batches = np.array(train_en_sequence_batches)\n",
    "train_target_batches = np.array(train_target_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_en_sequence_batches.npy', train_en_sequence_batches)\n",
    "np.save('train_target_batches.npy', train_target_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_batches = np.load('train_target_batches.npy')\n",
    "en_seq_batches = np.load('train_en_sequence_batches.npy')\n",
    "final_indexed_ques = pickle.load(open('final_indexed_ques.p', 'rb'))\n",
    "final_indexed_target = pickle.load(open('final_indexed_target.p', 'rb'))\n",
    "\n",
    "reverse_indexed_questions = []\n",
    "for batch in en_seq_batches:\n",
    "    bat = []\n",
    "    for ques in batch:\n",
    "        bat.append(ques[::-1])\n",
    "    reverse_indexed_questions.append(bat)\n",
    "reverse_indexed_questions = np.array(reverse_indexed_questions)\n",
    "\n",
    "\n",
    "word2idx = pickle.load(open('word2idx.p', 'rb'))\n",
    "idx2word = pickle.load(open('idx2word.p', 'rb'))\n",
    "max_len = 23\n",
    "batch_size = 15\n",
    "embedding_size=450\n",
    "num_layers=2\n",
    "word2idx.append('<GO>')\n",
    "vocab_size=len(word2idx)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(inputs, embedding_size, num_layers, vocab_size, seq_len_tensor):\n",
    "    \n",
    "    def cell(units, r):\n",
    "        layer = tf.contrib.rnn.BasicLSTMCell(units)\n",
    "        return tf.contrib.rnn.DropoutWrapper(layer, r)\n",
    "\n",
    "    encoder_cell = tf.nn.rnn_cell.MultiRNNCell([cell(embedding_size, 0.5) for _ in range(num_layers)])\n",
    "\n",
    "    encoder_embedings = tf.contrib.layers.embed_sequence(inputs, vocab_size, embedding_size)\n",
    "    \n",
    "    en_output, en_state = tf.nn.dynamic_rnn(encoder_cell, encoder_embedings, seq_len_tensor, dtype=tf.float32)\n",
    "    \n",
    "    return en_output, en_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(targets, batch_size, word2idx):\n",
    "     \n",
    "    endings = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1]) \n",
    "    processed_tar = tf.concat([tf.fill([batch_size, 1], word2idx.index('<GO>')), endings], 1)\n",
    "    return processed_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mechanism(embedding_size, en_output, en_state, encoder_decoder_seq_len, batch_size):\n",
    "    \n",
    "    \n",
    "    #https://skymind.ai/wiki/attention-mechanism-memory-network\n",
    "    \n",
    "    \n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(embedding_size)      \n",
    "    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = 0.5)\n",
    "    \n",
    "    attention = tf.contrib.seq2seq.BahdanauAttention(embedding_size, en_output, encoder_decoder_seq_len)\n",
    "    \n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention, embedding_size/2)\n",
    "    \n",
    "    attention_zero = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    enc_state_new = attention_zero.clone(cell_state=en_state[-1])\n",
    "    \n",
    "    return decoder_cell, enc_state_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(batch_size, embedding_size, dec_inputs, vocab_size, decoder_cell, enc_state_new, seq_len_tensor, max_seq_len):\n",
    "    \n",
    "    embedding_layer = tf.Variable(tf.random_uniform([vocab_size, embedding_size]))\n",
    "    embedding = tf.nn.embedding_lookup(embedding_layer, dec_inputs)\n",
    "    \n",
    "    output_layer = tf.layers.Dense(vocab_size, kernel_initializer=tf.truncated_normal_initializer(0.0, 0.1))\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('decoder'):\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(embedding, seq_len_tensor)\n",
    "        \n",
    "        train_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, \n",
    "                                                        train_helper, \n",
    "                                                        enc_state_new, \n",
    "                                                        output_layer)\n",
    "\n",
    "        train_dec_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(train_decoder, impute_finished=True, maximum_iterations=max_seq_len)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('decoder', reuse=True):\n",
    "        \n",
    "        starting_id_vec = tf.tile(tf.constant([word2idx.index('<GO>')], dtype=tf.int32), [batch_size], name='starting_id_vec')\n",
    "        \n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_layer, \n",
    "                                                                    starting_id_vec, \n",
    "                                                                    word2idx.index('<EOS>'))\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,\n",
    "                                                            inference_helper, \n",
    "                                                            enc_state_new, \n",
    "                                                            output_layer)\n",
    "        \n",
    "        \n",
    "        inference_dec_output, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, impute_finished=True, maximum_iterations=max_seq_len)\n",
    "        \n",
    "    return train_dec_outputs, inference_dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_optimizer(train_outputs, encoder_decoder_seq_len, max_seq_len, targets, learning_rate=0.0001, clip_rate=4):\n",
    "    \n",
    "    logits = tf.identity(train_outputs.rnn_output)\n",
    "    \n",
    "    mask_weights = tf.sequence_mask(encoder_decoder_seq_len, max_seq_len, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('lo_op'):\n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, targets, mask_weights)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "        clipped_grads, _ = tf.clip_by_global_norm(gradients, clip_rate)\n",
    "        traiend_opt = opt.apply_gradients(zip(clipped_grads, tf.trainable_variables()))\n",
    "        \n",
    "    return loss, traiend_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "inputs =  tf.placeholder(tf.int32, [None, None])\n",
    "targets = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_decoder_seq_len = tf.placeholder(tf.int32, (None,))\n",
    "max_seq_len = tf.reduce_max(encoder_decoder_seq_len, name='max_seq_len')\n",
    "\n",
    "en_output, en_state = encoding_layer(inputs, embedding_size, num_layers, vocab_size, encoder_decoder_seq_len)\n",
    "\n",
    "dec_inputs = process_targets(targets, batch_size, word2idx)\n",
    "\n",
    "decoder_cell, enc_state_new = attention_mechanism(embedding_size, en_output, en_state, encoder_decoder_seq_len, batch_size)\n",
    "\n",
    "train_outputs, inference_outputs = decoding_layer(batch_size, embedding_size, dec_inputs, vocab_size, decoder_cell, enc_state_new, encoder_decoder_seq_len, max_seq_len)\n",
    "\n",
    "predictions  = tf.identity(inference_outputs.sample_id, name='preds')\n",
    "\n",
    "loss, opt = loss_optimizer(train_outputs, encoder_decoder_seq_len, max_seq_len, targets, learning_rate=0.0001, clip_rate=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('checkpoint/')\n",
    "\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        module_file = tf.train.latest_checkpoint('checkpoint/')\n",
    "        saver.restore(sess, module_file)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        total_loss = 0\n",
    "\n",
    "        display_counter = 700\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0 \n",
    "            starting_time = time.time()\n",
    "            \n",
    "            for x_bat, y_bat in zip(reverse_indexed_questions, tar_batches):\n",
    "\n",
    "                    \n",
    "                    batch_index+=1\n",
    "\n",
    "                    _, loss_val, ppp = sess.run([opt, loss, predictions], {inputs:x_bat, targets:y_bat,encoder_decoder_seq_len:[len(x_bat[0])]*batch_size})\n",
    "\n",
    "                    total_loss+=loss_val\n",
    "\n",
    "                    if batch_index%display_counter==0:\n",
    "                        batch_time = time.time() - starting_time\n",
    "                        print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'.format(epoch+1, epochs, batch_index, len(en_seq_batches), total_loss/display_counter, batch_time))\n",
    "                        saver.save(sess, 'checkpoint/model')\n",
    "                        starting_time = time.time()\n",
    "                        total_loss=0\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted by user')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "                     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    module_file = tf.train.latest_checkpoint('checkpoint/')\n",
    "    saver.restore(sess, module_file)\n",
    "    \n",
    "    \n",
    "    rand_ques_no = random.randint(0,len(final_indexed_ques))\n",
    "    rand_ques = final_indexed_ques[rand_ques_no]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\nQuestion:',end=\" \")\n",
    "    for id_ in rand_ques:\n",
    "        if idx2word[id_]=='<PAD>':\n",
    "            break\n",
    "        print(idx2word[id_],end=\" \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    fake_bat = np.zeros((batch_size, max_len))\n",
    "    fake_bat[0] = ques\n",
    "    en_dc_len = np.ones(batch_size, dtype=np.int32)\n",
    "    en_dc_len[0] = max_len    #determines the length of output for the fake_bat[0], which is nothing but a question\n",
    "    \n",
    "    \n",
    "    pred = sess.run([predictions], feed_dict={inputs: fake_bat, encoder_decoder_seq_len:en_dc_len})[0][0]\n",
    "    \n",
    "    print('\\nAnswer:',end=\" \")\n",
    "    for id_ in pred:\n",
    "        if idx2word[id_]=='<EOS>':\n",
    "            break\n",
    "        print(idx2word[id_],end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
